import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch.helpers import async_bulk
import numpy as np
import warnings
import io
from service.utils.helpers import sanitize_for_es

warnings.filterwarnings("ignore", category=UserWarning)

PRODUCTS_INDEX = "products"
SERVICES_INDEX = "services"
ACCESSORIES_INDEX = "accessories"

def get_shared_index_mapping(data_type: str):
    """
    Tr·∫£ v·ªÅ mapping cho m·ªôt lo·∫°i d·ªØ li·ªáu c·ª• th·ªÉ, ƒë√£ bao g·ªìm tr∆∞·ªùng 'customer_id'.
    """
    common_properties = {
        "customer_id": {"type": "keyword"}
    }
    if data_type == "product":
        specific_properties = {
            "ma_san_pham": {"type": "keyword"},
            "model": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "mau_sac": {"type": "keyword"},
            "dung_luong": {"type": "keyword"},
            "tinh_trang_may": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "loai_thiet_bi": {"type": "keyword"},
            "gia": {"type": "double"}, "ton_kho": {"type": "integer"},
        }
    elif data_type == "service":
        specific_properties = {
            "ma_dich_vu": {"type": "keyword"},
            "ten_dich_vu": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "ten_san_pham": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "gia": {"type": "double"},
        }
    elif data_type == "accessory":
        specific_properties = {
            "accessory_code": {"type": "keyword"},
            "accessory_name": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "category": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "lifecare_price": {"type": "double"}, "inventory": {"type": "integer"},
        }
    else:
        return {}
        
    common_properties.update(specific_properties)
    return {"properties": common_properties}

async def ensure_shared_indices_exist(es_client: Elasticsearch):
    """
    Ki·ªÉm tra v√† t·∫°o c√°c index chia s·∫ª n·∫øu ch√∫ng ch∆∞a t·ªìn t·∫°i.
    """
    indices_to_create = {
        PRODUCTS_INDEX: "product",
        SERVICES_INDEX: "service",
        ACCESSORIES_INDEX: "accessory"
    }
    for index_name, data_type in indices_to_create.items():
        if not await es_client.indices.exists(index=index_name):
            print(f"üõ† ƒêang t·∫°o index chia s·∫ª '{index_name}'...")
            mapping = get_shared_index_mapping(data_type)
            await es_client.indices.create(index=index_name, mappings=mapping)
            print(f"‚úÖ T·∫°o th√†nh c√¥ng index '{index_name}'.")

async def clear_customer_data(es_client: Elasticsearch, index_name: str, customer_id: str):
    """
    X√≥a t·∫•t c·∫£ d·ªØ li·ªáu c·ªßa m·ªôt customer_id c·ª• th·ªÉ kh·ªèi m·ªôt index.
    """
    print(f"üóëÔ∏è ƒêang x√≥a d·ªØ li·ªáu c≈© c·ªßa kh√°ch h√†ng '{customer_id}' trong index '{index_name}'...")
    try:
        await es_client.delete_by_query(
            index=index_name,
            query={"term": {"customer_id": customer_id}},
            refresh=True,
            wait_for_completion=True
        )
        print(f"‚úÖ X√≥a d·ªØ li·ªáu c≈© th√†nh c√¥ng.")
    except Exception as e:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a d·ªØ li·ªáu c≈© (c√≥ th·ªÉ do ch∆∞a c√≥): {e}")

async def process_and_index_data(
    es_client: Elasticsearch, 
    customer_id: str,
    index_name: str, 
    file_content: bytes, 
    columns_config: dict
):
    """
    H√†m t·ªïng qu√°t ƒë·ªÉ ƒë·ªçc, x·ª≠ l√Ω v√† n·∫°p d·ªØ li·ªáu v√†o m·ªôt index chia s·∫ª.
    """
    await clear_customer_data(es_client, index_name, customer_id)

    try:
        df = pd.read_excel(io.BytesIO(file_content))
        df.columns = columns_config['names']
        df = df.dropna(subset=columns_config['required'])
        
        for col, dtype in columns_config.get('numerics', {}).items():
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(dtype)
            
        df['customer_id'] = customer_id
        df = df.where(pd.notnull(df), None).replace({np.nan: None})
    except Exception as e:
        raise ValueError(f"L·ªói ƒë·ªçc ho·∫∑c x·ª≠ l√Ω file Excel: {e}")

    actions = []
    for _, row in df.iterrows():
        doc = row.to_dict()
        action = {
            "_index": index_name,
            "_id": f"{customer_id}_{doc[columns_config['id_field']]}",
            "_source": doc,
            "routing": customer_id
        }
        actions.append(action)

    if not actions:
        return 0, 0

    print(f"üöÄ ƒêang n·∫°p {len(actions)} b·∫£n ghi v√†o index '{index_name}' cho kh√°ch h√†ng '{customer_id}'...")
    try:
        success, failed = await async_bulk(es_client, actions, raise_on_error=False, refresh=True)
        print(f"‚úÖ Th√†nh c√¥ng: {success} b·∫£n ghi.")
        if failed:
            print(f"‚ùå Th·∫•t b·∫°i: {len(failed)} b·∫£n ghi.")
        return success, len(failed)
    except Exception as e:
        raise IOError(f"L·ªói trong qu√° tr√¨nh bulk indexing: {e}")

async def index_single_document(es_client: Elasticsearch, index_name: str, customer_id: str, doc_id: str, doc_body: dict):
    """
    N·∫°p (ho·∫∑c ghi ƒë√®) m·ªôt b·∫£n ghi duy nh·∫•t v√†o index chia s·∫ª v·ªõi routing.
    """
    doc_body['customer_id'] = customer_id # Gi·ªØ customer_id g·ªëc trong source
    sanitized_customer_id = sanitize_for_es(customer_id)
    sanitized_doc_id = sanitize_for_es(doc_id)
    composite_id = f"{sanitized_customer_id}_{sanitized_doc_id}"
    
    try:
        response = await es_client.index(
            index=index_name,
            id=composite_id,
            document=doc_body,
            routing=sanitized_customer_id,
            refresh=True
        )
        return response
    except Exception as e:
        raise IOError(f"L·ªói khi n·∫°p b·∫£n ghi ƒë∆°n: {e}")

async def update_single_document(es_client: Elasticsearch, index_name: str, customer_id: str, doc_id: str, doc_body: dict):
    """
    C·∫≠p nh·∫≠t m·ªôt b·∫£n ghi duy nh·∫•t trong index chia s·∫ª.
    """
    sanitized_customer_id = sanitize_for_es(customer_id)
    sanitized_doc_id = sanitize_for_es(doc_id)
    composite_id = f"{sanitized_customer_id}_{sanitized_doc_id}"
    try:
        response = await es_client.update(
            index=index_name,
            id=composite_id,
            doc=doc_body,
            routing=sanitized_customer_id,
            refresh=True
        )
        return response
    except Exception as e:
        raise IOError(f"L·ªói khi c·∫≠p nh·∫≠t b·∫£n ghi: {e}")

async def delete_single_document(es_client: Elasticsearch, index_name: str, customer_id: str, doc_id: str):
    """
    X√≥a m·ªôt b·∫£n ghi duy nh·∫•t kh·ªèi index chia s·∫ª.
    """
    sanitized_customer_id = sanitize_for_es(customer_id)
    sanitized_doc_id = sanitize_for_es(doc_id)
    composite_id = f"{sanitized_customer_id}_{sanitized_doc_id}"
    try:
        response = await es_client.delete(
            index=index_name,
            id=composite_id,
            routing=sanitized_customer_id,
            refresh=True
        )
        return response
    except Exception as e:
        raise IOError(f"L·ªói khi x√≥a b·∫£n ghi: {e}")

async def bulk_index_documents(es_client: Elasticsearch, index_name: str, customer_id: str, documents: list[dict], id_field: str):
    """
    N·∫°p h√†ng lo·∫°t m·ªôt danh s√°ch c√°c b·∫£n ghi v√†o index chia s·∫ª.
    H√†m n√†y kh√¥ng x√≥a d·ªØ li·ªáu c≈©.
    """
    actions = []
    sanitized_customer_id = sanitize_for_es(customer_id)
    
    for doc in documents:
        doc_id = doc.get(id_field)
        if not doc_id:
            continue 
        
        doc['customer_id'] = customer_id
        sanitized_doc_id = sanitize_for_es(doc_id)
        composite_id = f"{sanitized_customer_id}_{sanitized_doc_id}"
        
        action = {
            "_index": index_name,
            "_id": composite_id,
            "_source": doc,
            "routing": sanitized_customer_id
        }
        actions.append(action)

    if not actions:
        return 0, 0

    try:
        success, failed = await async_bulk(es_client, actions, raise_on_error=False, refresh=True)
        return success, failed
    except Exception as e:
        raise IOError(f"L·ªói trong qu√° tr√¨nh bulk indexing h√†ng lo·∫°t: {e}")

async def process_and_upsert_file_data(
    es_client: Elasticsearch,
    customer_id: str,
    index_name: str,
    file_content: bytes,
    columns_config: dict
):
    """
    ƒê·ªçc file Excel, x·ª≠ l√Ω v√† N·∫†P TH√äM (upsert) d·ªØ li·ªáu v√†o index chia s·∫ª.
    H√†m n√†y KH√îNG x√≥a d·ªØ li·ªáu c≈© c·ªßa kh√°ch h√†ng.
    """
    try:
        df = pd.read_excel(io.BytesIO(file_content))
        df.columns = columns_config['names']
        df = df.dropna(subset=columns_config['required'])

        for col, dtype in columns_config.get('numerics', {}).items():
            df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(dtype)

        df = df.where(pd.notnull(df), None).replace({np.nan: None})
    except Exception as e:
        raise ValueError(f"L·ªói ƒë·ªçc ho·∫∑c x·ª≠ l√Ω file Excel: {e}")

    documents = df.to_dict('records')
    if not documents:
        return 0, 0

    success, failed = await bulk_index_documents(
        es_client,
        index_name,
        customer_id,
        documents,
        id_field=columns_config['id_field']
    )
    return success, failed
