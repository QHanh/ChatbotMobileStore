import os
import pandas as pd
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import warnings
from dotenv import load_dotenv
# from sentence_transformers import SentenceTransformer  # üëâ B·ªé COMMENT n·∫øu c·∫ßn t·∫°o embedding
import numpy as np
import json

load_dotenv()
warnings.filterwarnings("ignore", category=UserWarning)

# --- C·∫§U H√åNH ---
ELASTIC_HOST = "http://localhost:9200"
INDEX_NAME = "iphone_products"
XLSX_FILE_PATH = "iPhone.xlsm"

# --- K·∫æT N·ªêI ---
try:
    es_client = Elasticsearch(hosts=[ELASTIC_HOST])
    if not es_client.ping():
        raise ConnectionError("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Elasticsearch.")
    print("‚úÖ K·∫øt n·ªëi ƒë·∫øn Elasticsearch th√†nh c√¥ng!")
except (ValueError, ConnectionError) as e:
    print(f"‚ùå L·ªói: {e}")
    exit()

def create_index_mapping():
    if es_client.indices.exists(index=INDEX_NAME):
        print(f"‚ö†Ô∏è  Index '{INDEX_NAME}' ƒë√£ t·ªìn t·∫°i. X√≥a index c≈© ƒë·ªÉ t·∫°o l·∫°i.")
        es_client.indices.delete(index=INDEX_NAME)

    mapping = {
        "properties": {
            "ma_san_pham": {"type": "keyword"},
            "model": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
            "mau_sac": {"type": "keyword"},
            "dung_luong": {"type": "keyword"},
            "bao_hanh": {"type": "keyword"},
            "tinh_trang_may": {"type": "keyword"},
            "tinh_trang_pin": {"type": "float"},
            "gia": {"type": "double"},
            "ton_kho": {"type": "integer"},
            "ghi_chu": {"type": "text"},
            "ra_mat": {"type": "text"},
            "man_hinh": {"type": "text"},
            "chip_ram": {"type": "text"},
            "camera": {"type": "text"},
            "pin_mah": {"type": "text"},
            "ket_noi_hdh": {"type": "text"},
            "mau_sac_tieng_anh": {"type": "text"},
            "mau_sac_available": {"type": "text"},
            "dung_luong_available": {"type": "text"},
            "kich_thuoc_trong_luong": {"type": "text"},
            
            # üëâ N·∫øu c·∫ßn b·∫≠t t√¨m ki·∫øm ng·ªØ nghƒ©a, b·ªè comment b√™n d∆∞·ªõi
            # "text_embedding": {
            #     "type": "dense_vector",
            #     "dims": 384
            # }
        }
    }

    print(f"üõ† T·∫°o index m·ªõi '{INDEX_NAME}' v·ªõi mapping...")
    es_client.indices.create(index=INDEX_NAME, mappings=mapping)
    print("‚úÖ T·∫°o index th√†nh c√¥ng.")

def process_and_index_data():
    try:
        df = pd.read_excel(XLSX_FILE_PATH, sheet_name='TONGHOP')
        df.columns = [
            'ma_san_pham', 'model', 'mau_sac', 'dung_luong', 'bao_hanh',
            'tinh_trang_may', 'tinh_trang_pin', 'gia', 'ton_kho', 'ghi_chu',
            'ra_mat', 'man_hinh', 'chip_ram', 'camera', 'pin_mah', 'ket_noi_hdh',
            'mau_sac_tieng_anh', 'mau_sac_available', 'dung_luong_available',
            'kich_thuoc_trong_luong', 'bao_hanh_2'
        ]
        if 'bao_hanh_2' in df.columns:
            df = df.drop(columns=['bao_hanh_2'])

        df = df.dropna(subset=['ma_san_pham', 'model'])

        df['ton_kho'] = pd.to_numeric(df['ton_kho'], errors='coerce').fillna(0).astype(int)
        df['gia'] = pd.to_numeric(df['gia'], errors='coerce').fillna(0).astype(float)
        df['tinh_trang_pin'] = pd.to_numeric(df['tinh_trang_pin'], errors='coerce').fillna(0).astype(float)

        df['mau_sac'] = df['mau_sac'].astype(str).str.strip().str.title()

        df = df.where(pd.notnull(df), None).replace({np.nan: None})

    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file '{XLSX_FILE_PATH}'.")
        return
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc file Excel: {e}")
        return

    # üì¶ N·∫øu c·∫ßn d√πng embedding, b·ªè comment b√™n d∆∞·ªõi
    # print("üì¶ ƒêang t·∫£i model embedding...")
    # embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
    # print("‚úÖ T·∫£i model th√†nh c√¥ng.")

    actions = []
    total_rows = len(df)

    for index, row in df.iterrows():
        print(f"‚û°Ô∏è  ƒêang x·ª≠ l√Ω d√≤ng {index + 1}/{total_rows}: {row['model']}")

        # √âp to√†n b·ªô NaN ‚Üí None ƒë·ªÉ ƒë·∫£m b·∫£o JSON h·ª£p l·ªá
        doc = json.loads(json.dumps(row.to_dict(), default=lambda x: None))

        # --- T·∫†O VECTOR EMBEDDING CHO T√åM KI·∫æM NG·ªÆ NGHƒ®A ---
        # üëâ B·ªè comment n·∫øu c·∫ßn t·∫°o text_embedding
        # text_to_embed = f"ƒêi·ªán tho·∫°i {doc.get('model', '')} m√†u {doc.get('mau_sac', '')} dung l∆∞·ª£ng {doc.get('dung_luong', '')}. " \
        #               f"Th√¥ng s·ªë camera: {doc.get('camera', '')}. " \
        #               f"Th√¥ng s·ªë pin: {doc.get('pin_mah', '')}. " \
        #               f"Chip v√† RAM: {doc.get('chip_ram', '')}."

        # doc['text_embedding'] = embedding_model.encode(text_to_embed).tolist()

        action = {
            "_index": INDEX_NAME,
            "_id": doc['ma_san_pham'] + "_" + str(doc['ton_kho']),  # t·∫°o ID duy nh·∫•t
            "_source": doc
        }
        actions.append(action)

    print(f"\nüöÄ ƒêang index {len(actions)} s·∫£n ph·∫©m...")
    try:
        success, failed = bulk(es_client, actions, raise_on_error=False)
        print(f"‚úÖ Index th√†nh c√¥ng: {success} s·∫£n ph·∫©m.")
        if failed:
            print(f"‚ùå Index th·∫•t b·∫°i: {len(failed)} s·∫£n ph·∫©m.")
            for i, fail_info in enumerate(failed[:5]):
                error = fail_info.get('index', {}).get('error', {})
                print(f"  ‚ùå L·ªói {i+1}: {error.get('type', 'unknown')} - {error.get('reason', 'no reason')}")
    except Exception as e:
        print(f"‚ùå ƒê√£ x·∫£y ra l·ªói khi index d·ªØ li·ªáu: {e}")

if __name__ == "__main__":
    create_index_mapping()
    process_and_index_data()
    print("\n‚úÖ Qu√° tr√¨nh x·ª≠ l√Ω v√† index d·ªØ li·ªáu ƒë√£ ho√†n t·∫•t.")
